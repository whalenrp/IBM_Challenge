import sys
import math
import csv
from Queue import Queue
from AbstractLearner import AbstractLearner
from random import Random

class BaggingLearner(AbstractLearner):
	"""
	Derived class implementation of AbstractLearner. This class implements the learn() 
	and classify() functions using a Bagging approach
	"""

	def __init__(self, trainingInputFile, testInputFile, isMachineReadable, outputFile):
		AbstractLearner.__init__(self, trainingInputFile, testInputFile, 
			isMachineReadable, outputFile)

		# "forest" consists of a list of root nodes that represent trees. See
		# the Node class for the information a Node contains.
		self.forest = list()

		# The tree-building algorithm will halt when it sees a node with 
		# RECURSION_SPLIT_THRESHOLD number of items in it.
		self.RECURSION_SPLIT_THRESHOLD = math.sqrt(len(self.trainingData))

		# The number of trees generated by the bagging algorithm.
		self.NUMBER_OF_TREES = len(self.trainingData)/3
		
		#Probability of true required to classify as true
		self.TRUE_THRESHOLD = .5


	class Node:
		"""
		Container class for storing the percentage of Trues found at this node in the
		training data, the variable this node splits over (splitColumn), 
		the value that the variable splits around for the given column 
		(i.e. myRow[splitColumn] < splitValue ), and the left and right child themselves.

		If a node is a leaf node, its splitColumn value will equal -1
		"""
		def __init__(self, percentTrue, splitColumn=-1, splitValue=0, leftChild=None, rightChild=None):
			self.percentTrue = percentTrue
			self.splitColumn = splitColumn
			self.splitValue  = splitValue

			# The left and right child values will be "None" if this node is a leaf node.
			# It is also not possible to have a left child without a right child, since
			# that split would never have actually split the training data.
			self.leftChild   = leftChild 
			self.rightChild  = rightChild

		def printNode(self, nodeNumber, splitHeader):
			"""
			Prints the node with the following format:
			Node#: True_Probability Column_Identifier Split_Value
			"""
			nodeDescriptor = "Leaf_Node"
			if self.splitColumn is not -1:
				if splitHeader: 
					nodeDescriptor = splitHeader[self.splitColumn]
				else:
					nodeDescriptor = repr(self.splitColumn)
			
			print(repr(nodeNumber) + ': ' + repr(self.percentTrue) + ' ' + 
				nodeDescriptor + ' ' + repr(self.splitValue))
		

	def learn(self):
		"""
		Creates a classification model based on data held in the AbstractLearner's 
		trainingData list-of-lists
		"""

		del self.forest[:] # Empty the list of previous contents
		randomGen = Random()
		numRows = len(self.trainingData)
		for _ in range(self.NUMBER_OF_TREES): # The number of random trees in forest
			data = list()
			for _ in range(numRows): # Generate the random tree itself
				randomNum = randomGen.randrange(numRows)
				data.append(self.trainingData[randomNum])
			self.forest.append(
				self.makeTree(data, range(len(self.trainingData)),self.RECURSION_SPLIT_THRESHOLD))
#		self.makeTree(self.trainingData, range(len(self.trainingData)), self.RECURSION_SPLIT_THRESHOLD)
		

	def classify(self):
		"""
		Based on the classification model generated by learn(), this function will read from
		the testData list-of-lists in AbstractLearner and output the prediction for each 
		variable
		"""
		#create a csv writer for output
		myWriter = csv.writer(open(self.outputFile, "wb"))
		numRows = len(self.testData)
		#iterate over each row in the test data
		for i in range(numRows):
			#variables to hold how many votes for true and false are given
			trueVote = 0
			falseVote = 0
			#classify the row with each tree
			for j in range(self.NUMBER_OF_TREES):
				#call classifyTree to classify a specific row with a specific tree
				curVote = self.classifyTree(self.testData[i], self.forest[j])
				#add the vote, either true or false to the count
				if curVote == True:
					trueVote += 1
				else:
					falseVote += 1
			#if there are more true votes, write the row to the csv file
			if trueVote >= falseVote:
				myWriter.writerow([i])
				print i
		sys.exit(0)

	
	def classifyTree(self, myRow, myTree):
		"""
		Given a row of data and a tree from the forest, classify the row as either true
		or false based on the decision tree.
		"""
		#create a breadth-first-search queue and add the root node to it
		bfsQueue = Queue()
		bfsQueue.put(myTree)
		#search the decision tree
		while not bfsQueue.empty():
			#get the node off the top of the queue
			curNode = bfsQueue.get()
			#if it is not a leaf
			if curNode.splitColumn is not -1:
				#add the left branch to the queue if the value in the row is less than the
				#splitValue in the node, otherwise add the right branch to the queue
				if myRow[curNode.splitColumn] < curNode.splitValue:
					bfsQueue.put(curNode.leftChild)
				else:
					bfsQueue.put(curNode.rightChild)
			#if it is a leaf, reached the end of the decision tree for this row
			else:
				#return false if the probability of true is less than the set threshold
				#otherwise return true
				if curNode.percentTrue < self.TRUE_THRESHOLD:
					return False
				else:
					return True


	def makeTree(self, data, rangeOfRows, recursionThreshold):
		"""
		Returns a tree created from the given data.
		"""
		numTrues = 0
		for i in rangeOfRows:
			if data[i][-1]: numTrues += 1

		# Base case. If we have fewer than recursionThreshold rows to split around,
		# terminate.
		if len(rangeOfRows) <= recursionThreshold:
			return self.Node(numTrues/len(rangeOfRows))

		# bestOverallSplit contains (column, splitValue, number of Trues)
		(splitColumn, splitValue, numTrues) = self.findBestSplit(data, rangeOfRows)

		# Base case. If there is no good column to split around or
		# no more splitting should be done, terminate
		if numTrues == 0 or numTrues == len(rangeOfRows):
			return self.Node(numTrues/len(rangeOfRows))

		# Build the range of training rows that will go left or right
		leftSplit = list()
		rightSplit = list()
		for i in rangeOfRows:
			if data[i][splitColumn] < splitValue:
				leftSplit.append(i)
			else:
				rightSplit.append(i)

		# If the split puts all data into either the left or the right child,
		# we aren't actually splitting. Terminate, since no good split was found 
		# to be better than random.
		if len(leftSplit) == 0 or len(rightSplit) == 0:
			return self.Node(numTrues/len(rangeOfRows))
		
		return self.Node(float(numTrues)/ len(rangeOfRows), # Percentage true
					splitColumn, # Column to split the data around for children
					splitValue, # Value to split on splitColumn
					self.makeTree(data, leftSplit, recursionThreshold), # recurse for left child
					self.makeTree(data, rightSplit, recursionThreshold)) # recurse for right child


	def findBestSplit(self, data, rangeOfRows):
		"""
		Helper function that calculates the best variable to split around and
		what value to split on. This function also returns the number of Trues
		in the result. 
		Returns (splitIndex, splitValue, trueCount)
		"""
		# Pull out only the data that is important for this node
		nodeData = [data[x] for x in rangeOfRows]
		
		# transpose data
		columns = [list(a) for a in zip(*nodeData)]

		# Get the number of trues from the last column
		totalTrueCount = len(filter(lambda x: x, columns[-1]))
		
		#self.printListofLists(columns)
		
		# Zip up the continuously valued variables with their classifications and
		# sort by the continuous variables. Then, find the best split of the row
		# for each variable and decide to split on the best variable overall.
		bestSplitOverall = (-1, 0, 0) # (column, splitValue, score)
		for i in range(len(columns)-1):
			tuplesList = sorted(zip(columns[i], columns[-1]))

			# First, calculate the "sum" of this row of entries from the 
			# number of Trues and Falses in the row. We will use this
			# value to check if the majority of Trues or falses are to the
			# left or right of a possible split.
			totalRowValue = 0
			for mDataPoint in tuplesList:
				totalRowValue += 1 if mDataPoint[1] else -1

			# loop over the tuples, but only consider splits
			# that are between different numbers. (ie. don't 
			# split between two equal values)
			bestSplit = (0, abs(totalRowValue)) # The best split so far is tracked by (index, score)
			curScore = 0
			prevElement = tuplesList[0][0]
			for j in range(1, len(tuplesList)):
				curScore += 1 if tuplesList[j-1][1] else -1
				scoreRightOfSplit = totalRowValue - curScore

				# If we are looking at a new element and the current score is better
				# than any previous score, update our best score
				if tuplesList[j][0] != prevElement and abs(curScore - scoreRightOfSplit) > bestSplit[1]:
					bestSplit = (j, abs(curScore - scoreRightOfSplit))
				prevElement = tuplesList[j][0]

			# Set the bestSplitOverall if this is our first iteration
			# so we have a value to beat
			if i == 0:
				bestSplitOverall = (0, tuplesList[bestSplit[0]][0], bestSplit[1])
			elif bestSplit[1] > bestSplitOverall[2]:
				bestSplitOverall = (i, tuplesList[bestSplit[0]][0], bestSplit[1])
			#print(tuplesList, bestSplit, bestSplitOverall)
		return (bestSplitOverall[0], bestSplitOverall[1], totalTrueCount)

	def printHumanReadableTree(self, rootNode=None):
		print("Human Readble trees are printed as follows:")
		print("  Node#: True_Probability Column_Identifier Split_Value\n")
		print("  Node# represents the Breadth-First-Search number of this node")
		print("  True_Probability is the likelihood that a piece of data with this")
		print("	classification is true")
		print("  Column_Identifier is the human-readable header for the column this node")
		print("	will split over")
		print("  Split_Value is the value used for splitting. All entries with value")
		print("	< Split_Value will be classified to the left child.\n")

		if rootNode == None:
			rootNode = self.forest[0]

		bfsQueue = Queue()
		bfsQueue.put(rootNode)
		nodeNumber = 1
		while not bfsQueue.empty():
			frontNode = bfsQueue.get()

			# The node class is a leaf is its splitColumn is invalid
			if frontNode.splitColumn is not -1:
				bfsQueue.put(frontNode.leftChild)
				bfsQueue.put(frontNode.rightChild)
				frontNode.printNode(nodeNumber, self.headerList)
			else:
				frontNode.printNode(nodeNumber, "Leaf Node")
			nodeNumber += 1




